{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13091b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "4702639 examples loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import datasets\n",
    "\n",
    "fin = open('/home/mcwave/code/automath/atp/datasets/prover_training_data_6.json', \"r\")\n",
    "data = json.load(fin)\n",
    "print(\"Data loaded\")\n",
    "data_2 = []\n",
    "\n",
    "#Load input data\n",
    "for example in data:\n",
    "    example[-1] = example[-1].split(\"\\n\")[0]\n",
    "    data_2.append(tuple(example))\n",
    "\n",
    "data = data_2\n",
    "random.shuffle(data)\n",
    "print(len(data), \"examples loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b4396d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_name = \"morph-labs/morph-prover-v0-7b\" #\"internlm/internlm2-math-7b\" #\"ScalableMath/Lean-STaR-plus\"  # 'Saisam/gpt-neo-math-small' #\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Define the separator token\n",
    "sep_token = \"<sep>\"\n",
    "pad_token = \"<pad>\"\n",
    "eos_token = \"<end>\"\n",
    "\n",
    "# Check if the separator token already exists in the vocabulary\n",
    "if sep_token not in tokenizer.get_vocab():\n",
    "    tokenizer.add_tokens([sep_token])\n",
    "if pad_token not in tokenizer.get_vocab():\n",
    "    tokenizer.add_tokens([pad_token])\n",
    "\n",
    "# Set the separator token\n",
    "tokenizer.sep_token = sep_token\n",
    "tokenizer.pad_token = pad_token\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "    'sep_token': sep_token,\n",
    "    'pad_token': pad_token,\n",
    "    'eos_token': eos_token\n",
    "})\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"kaiyuy/leandojo-lean4-tacgen-byt5-small\")\n",
    "# tokenizer.sep_token = tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f5651df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0005257129669189453\n",
      "10000 17.104671955108643\n",
      "20000 33.486328125\n",
      "30000 50.46138286590576\n",
      "40000 66.78817486763\n",
      "50000 83.59446382522583\n",
      "60000 100.31472396850586\n",
      "70000 116.90418672561646\n",
      "80000 133.41736149787903\n",
      "90000 150.0275523662567\n"
     ]
    }
   ],
   "source": [
    "# state_pps = []\n",
    "# tacs = []\n",
    "\n",
    "# #Tokenize input data\n",
    "# start_time = time.time()\n",
    "# data_len = len(data)\n",
    "# for i in range(100000):\n",
    "#     state_pp = data[i][0]\n",
    "#     tac = data[i][1]\n",
    "#     tokens = tokenizer.encode(state_pp)\n",
    "#     if len(tokens) < 256:\n",
    "# #         print(state_pp)\n",
    "# #         print(tokenizer.decode(tokens[1:]))\n",
    "# #         break\n",
    "#         if tokenizer.decode(tokens[1:]) == state_pp: \n",
    "#             state_pps.append(state_pp)\n",
    "#             tacs.append(tac)\n",
    "#     if i % 10000 == 0:\n",
    "#         print(i, time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e371e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fin = open('/home/mcwave/code/automath/atp/ReProver/data/leandojo_benchmark_4/novel_premises/train.json', \"r\")\n",
    "# data = json.load(fin)\n",
    "# state_pps = []\n",
    "# tacs = []\n",
    "\n",
    "# for ex in data:\n",
    "#     try:\n",
    "#         state_info = ex[\"traced_tactics\"]\n",
    "#         for tac in state_info:\n",
    "#             state_pps.append(tac[\"state_before\"])\n",
    "#             tacs.append(tac[\"tactic\"])\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85c1c494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac995720da6f4435ad9ea00015fde84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/246714 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "# Assuming you have two lists: instructions and responses\n",
    "\n",
    "instructions = state_pps\n",
    "responses = tacs\n",
    "\n",
    "# Function to tokenize and prepare the data\n",
    "def prepare_data(examples):\n",
    "    # Concatenate instruction and response with a separator\n",
    "    full_texts = [f\"{instruction} <sep> {response}\" for instruction, response in zip(examples['instruction'], examples['response'])]\n",
    "#     print(full_texts[0])\n",
    "#     print(1/0)\n",
    "    # Tokenize the full texts\n",
    "    encodings = tokenizer(full_texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors='pt')\n",
    "    #print(encodings)\n",
    "    \n",
    "    # Create attention masks: 1 for response tokens, 0 for instruction tokens and padding\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "    it = 0\n",
    "\n",
    "    for input_ids in encodings['input_ids']:\n",
    "        attention_mask = torch.zeros_like(input_ids)\n",
    "        label = input_ids.clone()\n",
    "        \n",
    "        pad_token_idx = (input_ids == tokenizer.pad_token_id).nonzero()\n",
    "        end_idx = pad_token_idx[0].item() if len(pad_token_idx) > 0 else len(input_ids)\n",
    "        sep_token_idx = (input_ids == tokenizer.sep_token_id).nonzero()\n",
    "        #print(\"sep_token_idx:\", sep_token_idx)\n",
    "        if len(sep_token_idx) == 0:\n",
    "            sep_token_idx = 0\n",
    "        else:\n",
    "            sep_token_idx = sep_token_idx.item()\n",
    "\n",
    "        attention_mask[0:end_idx+1] = 1\n",
    "        attention_masks.append(attention_mask)\n",
    "        \n",
    "        label[0:sep_token_idx] = -100\n",
    "        labels.append(label)\n",
    "#         print(input_ids)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': encodings['input_ids'],\n",
    "        'attention_mask': torch.stack(attention_masks),\n",
    "        'labels': torch.stack(labels)\n",
    "    }\n",
    "\n",
    "# Create the Hugging Face dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    'instruction': instructions,\n",
    "    'response': responses\n",
    "})\n",
    "\n",
    "# Apply the tokenization and preparation function\n",
    "tokenized_dataset = dataset.map(\n",
    "    prepare_data,\n",
    "    batched=True\n",
    "    #remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf4e11e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61fd0579eac24b8a89d84d729c8a6d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/246714 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44229e097fd4900ab3b5e7ae91750c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/246714 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column train not in the dataset. Current columns in the dataset: ['instruction', 'response', 'input_ids', 'attention_mask', 'labels']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# tokenized_dataset = tokenized_dataset_2.train_test_split(train_size=0.95)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_from_disk(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets/reprover_dataset.dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m tokenized_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtokenized_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     23\u001b[0m tokenized_test_dataset \u001b[38;5;241m=\u001b[39m tokenized_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/atp/lib/python3.10/site-packages/datasets/arrow_dataset.py:2861\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2861\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/atp/lib/python3.10/site-packages/datasets/arrow_dataset.py:2845\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2843\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2844\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2845\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2846\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2847\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2848\u001b[0m )\n\u001b[1;32m   2849\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/anaconda3/envs/atp/lib/python3.10/site-packages/datasets/formatting/formatting.py:584\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    582\u001b[0m         _raise_bad_key_type(key)\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 584\u001b[0m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    586\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n",
      "File \u001b[0;32m~/anaconda3/envs/atp/lib/python3.10/site-packages/datasets/formatting/formatting.py:521\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[0;32m--> 521\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column train not in the dataset. Current columns in the dataset: ['instruction', 'response', 'input_ids', 'attention_mask', 'labels']\""
     ]
    }
   ],
   "source": [
    "# def add_end_token(example):\n",
    "#     end_idx = 0\n",
    "#     while end_idx < len(example[\"attention_mask\"]) - 1 and example[\"attention_mask\"][end_idx] == 1:\n",
    "#         end_idx += 1\n",
    "#     example[\"input_ids\"][end_idx-1] = tokenizer.eos_token_id\n",
    "#     example[\"labels\"][end_idx-1] = tokenizer.eos_token_id\n",
    "#     for i in range(len(example[\"input_ids\"])):\n",
    "# #         print(example[\"input_ids\"][i])\n",
    "#         if example[\"input_ids\"][i] == tokenizer.sep_token_id:\n",
    "#             example[\"labels\"][i] = tokenizer.sep_token_id\n",
    "#             break\n",
    "# #     print(example[\"input_ids\"])\n",
    "# #     print(1/0)\n",
    "#     return example\n",
    "\n",
    "# tokenized_dataset_2 = tokenized_dataset.map(add_end_token)\n",
    "\n",
    "# tokenized_dataset_2.save_to_disk(\"datasets/reprover_dataset.dataset\")\n",
    "\n",
    "tokenized_dataset = datasets.load_from_disk(\"datasets/reprover_dataset.dataset\")\n",
    "tokenized_dataset = tokenized_dataset_2.train_test_split(train_size=0.95)\n",
    "tokenized_train_dataset = tokenized_dataset[\"train\"]\n",
    "tokenized_test_dataset = tokenized_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0202b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 -100\n",
      "264 1 -100\n",
      "287 1 -100\n",
      "714 1 -100\n",
      "12082 1 -100\n",
      "13 1 -100\n",
      "2799 1 -100\n",
      "714 1 -100\n",
      "264 1 -100\n",
      "13 1 -100\n",
      "28716 1 -100\n",
      "28726 1 -100\n",
      "714 1 -100\n",
      "28705 1 -100\n",
      "29873 1 -100\n",
      "28726 1 -100\n",
      "13 1 -100\n",
      "28716 1 -100\n",
      "28721 1 -100\n",
      "28722 1 -100\n",
      "714 1 -100\n",
      "12082 1 -100\n",
      "13 1 -100\n",
      "229 1 -100\n",
      "141 1 -100\n",
      "165 1 -100\n",
      "28705 1 -100\n",
      "29873 1 -100\n",
      "28726 1 -100\n",
      "28705 1 -100\n",
      "32000 1 32000\n",
      "28705 1 28705\n",
      "1290 1 1290\n",
      "28720 1 28720\n",
      "733 1 733\n",
      "28716 1 28716\n",
      "28726 1 28726\n",
      "28793 1 28793\n",
      "32002 1 32002\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n",
      "32001 0 32001\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m test_data \u001b[38;5;241m=\u001b[39m tokenized_train_dataset[\u001b[38;5;241m40\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m512\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m][i], test_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m][i])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "test_data = tokenized_train_dataset[40]\n",
    "for i in range(512):\n",
    "    print(test_data[\"input_ids\"][i], test_data[\"attention_mask\"][i], test_data[\"labels\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db083ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_dataset = datasets.load_from_disk(\"datasets/atp_dataset_0821.dataset\").train_test_split(train_size=0.95)\n",
    "# tokenized_train_dataset = tokenized_dataset[\"train\"]\n",
    "# tokenized_test_dataset = tokenized_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ed80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn.init as init\n",
    "from transformers import AutoModel\n",
    "\n",
    "def initialize_weights(model, init_type='xavier'):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            if init_type == 'xavier':\n",
    "                init.xavier_uniform_(module.weight)\n",
    "            elif init_type == 'he':\n",
    "                init.kaiming_uniform_(module.weight, nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.fill_(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'models/trained_model_gptneo350m-2/checkpoint-100000',\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "                \n",
    "#model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\").to(\"cuda\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"data/results-gptneo350m-2/checkpoint-545000\").to(\"cuda\") #\"EleutherAI/gpt-neo-1.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a8945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset,load_metric\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/trained_model_gptneo350m-2\",\n",
    "    evaluation_strategy=\"steps\", #\"epochs\"\n",
    "    learning_rate=2e-5,  # PAY ATTENTION TO LEARNING RATE!\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=4,\n",
    "    bf16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    save_steps=20000,\n",
    "    eval_steps=20000,\n",
    "    logging_steps=1000,\n",
    "    save_total_limit=3,\n",
    "    #load_best_model_at_end=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "cp_path = 'models/trained_model_gptneo350m-2/checkpoint-100000'\n",
    "\n",
    "trainer.train(cp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d48c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"models/prover-gemma2-v2/checkpoint-350000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6897e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AdamW\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "class InstructionResponseDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instruction, response = self.data[idx]\n",
    "        \n",
    "        input_encoding = self.tokenizer(instruction, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        output_encoding = self.tokenizer(response, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        input_mask = input_encoding[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": input_mask,\n",
    "            \"labels\": output_encoding[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "model_name = \"google/byt5-base\"  # You can change this to any other Seq2Seq model\n",
    "model_name = \"./models/trained_model_base_256k\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kaiyuy/leandojo-lean4-tacgen-byt5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# # Prepare your data\n",
    "# data = [\n",
    "#     (\"Write a poem about spring.\", \"Blossoms bloom in gentle breeze,\\nSunshine warms the earth with ease.\\nBirds return with joyful song,\\nDays grow longer, winter's gone.\"),\n",
    "#     (\"Explain photosynthesis.\", \"Photosynthesis is the process by which plants use sunlight, water, and carbon dioxide to produce oxygen and energy in the form of sugar.\"),\n",
    "#     (\"What is the capital of France?\", \"The capital of France is Paris.\"),\n",
    "#     (\"Describe the water cycle.\", \"The water cycle involves evaporation, condensation, precipitation, and collection of water on Earth's surface and in the atmosphere.\"),\n",
    "#     # Add more instruction-response pairs here\n",
    "# ]\n",
    "\n",
    "# Create full dataset\n",
    "full_dataset = InstructionResponseDataset(data, tokenizer)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=6, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=6, shuffle=False)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7280dfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#     print(1,i,train_dataset[i][\"input_ids\"].shape)\n",
    "#     print(2,i,train_dataset[i][\"attention_mask\"].shape)\n",
    "#     print(3,i,train_dataset[i][\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7423d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Save the model after each epoch\n",
    "    save_path = f\"./models/trained_model_epoch_{epoch + 1}\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    model.save_pretrained(save_path)\n",
    "    tozzkenizer.save_pretrained(save_path)\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    avg_test_loss = evaluate(model, test_dataloader, device)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Average Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Average Test Loss: {avg_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ed6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"./models/trained_model_base_343k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e885af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "def generate_response(instruction):\n",
    "    input_ids = tokenizer(instruction, return_tensors=\"pt\").input_ids.to(device)\n",
    "    print(input_ids)\n",
    "    output = model.generate(input_ids, max_length=200)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "test_instruction = \"\"\"a b c : ℝ\n",
    "⊢ a * (c * b) = b * (a * c)\n",
    "\"\"\"\n",
    "\n",
    "response = generate_response(test_instruction)\n",
    "print(f\"Instruction: {test_instruction}\")\n",
    "print(f\"Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atp",
   "language": "python",
   "name": "atp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
